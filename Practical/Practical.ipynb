{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Course Homework: Machine Learning Fundamentals\n",
    "\n",
    "This notebook provides hands-on experience with key machine learning concepts: **Naive Bayes**, **Decision Trees**, **Regression (Polynomial and Logistic)**, **Neural Networks** and **CNN**. You'll implement algorithms from scratch and apply them to datasets to deepen your understanding. The notebook is divided into five sections, each with a theoretical overview and practical exercises.\n",
    "\n",
    "**Instructions:**\n",
    "- Complete all sections in the code and answer the questions.\n",
    "- Submit the notebook with all code cells executed and outputs visible.\n",
    "- Use `numpy` for basic operations, but implement core algorithm logic yourself where indicated\n",
    "\n",
    "\n",
    "\n",
    "## Sections\n",
    "1. **Naive Bayes**\n",
    "2. **Decision Trees**\n",
    "3. **Regression (Polynomial and Logistic)**\n",
    "4. **Neural Networks (Perceptron and Multi-Layer Perceptron)**\n",
    "5. **CNN From Scratch!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name=\"\"\n",
    "Student_Number=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Naive Bayes\n",
    "\n",
    "### Theoretical Overview\n",
    "**Naive Bayes** is a probabilistic classifier that assumes features are conditionally independent given the class label, using Bayes' theorem:\n",
    "\n",
    "$$ P(Y | F_1, ..., F_n) \\propto P(Y) \\prod_{i} P(F_i | Y) $$\n",
    "\n",
    "Here, you'll implement Naive Bayes for spam classification.\n",
    "\n",
    "### Practical Exercise: Implementing Naive Bayes\n",
    "**Dataset:** Emails labeled 'spam' or 'ham' with binary features 'free' and 'money'.\n",
    "\n",
    "| free | money | label |\n",
    "|------|-------|-------|\n",
    "| 1    | 0     | spam  |\n",
    "| 0    | 1     | spam  |\n",
    "| 0    | 0     | ham   |\n",
    "| 1    | 1     | spam  |\n",
    "| 0    | 0     | ham   |\n",
    "\n",
    "**Tasks:**\n",
    "1. Calculate prior probabilities $ P(\\text{spam}) $ and $ P(\\text{ham}) $.\n",
    "2. Calculate conditional probabilities with Laplace smoothing ($ k=1 $): $ P(\\text{free}=1 | \\text{spam}) $, $ P(\\text{free}=1 | \\text{ham}) $, $ P(\\text{money}=1 | \\text{spam}) $, $ P(\\text{money}=1 | \\text{ham}) $.\n",
    "3. Predict the label for a new email: $ \\text{free}=1 $, $ \\text{money}=0 $.\n",
    "\n",
    "**Questions:**\n",
    "- Why is Laplace smoothing important?\n",
    "  - **Answer:** YOUR ANSWER HERE\n",
    "- How does feature independence affect performance?\n",
    "  - **Answer:** YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Toy dataset\n",
    "data = np.array([\n",
    "    [1, 0, 'spam'],\n",
    "    [0, 1, 'spam'],\n",
    "    [0, 0, 'ham'],\n",
    "    [1, 1, 'spam'],\n",
    "    [0, 0, 'ham']\n",
    "])\n",
    "X = data[:, :-1].astype(int)\n",
    "y = data[:, -1]\n",
    "\n",
    "# Calculate prior probabilities\n",
    "total = len(y)\n",
    "p_spam = ...\n",
    "p_ham = ...\n",
    "print(f'P(spam) = {p_spam}, P(ham) = {p_ham}')\n",
    "\n",
    "# Calculate conditional probabilities with Laplace smoothing\n",
    "def conditional_prob(feature_idx, value, label, X, y, k=1):\n",
    "    ...\n",
    "\n",
    "p_free1_spam = conditional_prob(0, 1, 'spam', X, y)\n",
    "p_free1_ham = conditional_prob(0, 1, 'ham', X, y)\n",
    "p_money1_spam = conditional_prob(1, 1, 'spam', X, y)\n",
    "p_money1_ham = conditional_prob(1, 1, 'ham', X, y)\n",
    "print(f'P(free=1|spam) = {p_free1_spam}, P(free=1|ham) = {p_free1_ham}')\n",
    "print(f'P(money=1|spam) = {p_money1_spam}, P(money=1|ham) = {p_money1_ham}')\n",
    "\n",
    "# Predict for new email\n",
    "new_email = np.array([1, 0])\n",
    "p_spam_norm = ...\n",
    "p_ham_norm = ...\n",
    "prediction = ...\n",
    "print(f'P(spam|free=1, money=0) = {p_spam_norm}, P(ham|free=1, money=0) = {p_ham_norm}')\n",
    "print(f'Prediction: {prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Decision Trees\n",
    "\n",
    "### Theoretical Overview\n",
    "**Decision Trees** split data based on features to maximize **information gain**:\n",
    "\n",
    "$$ IG(X_i) = H(Y) - H(Y | X_i) $$\n",
    "$$ H(Y) = -\\sum_c P(Y=c) \\log_2 P(Y=c) $$\n",
    "\n",
    "where $ H $ is entropy. Overfitting is a risk, addressed by pruning.\n",
    "\n",
    "### Practical Exercise: Implementing a Decision Tree\n",
    "**Dataset:** Features $ \\text{age} $ (young/old), $ \\text{income} $ (low/high), $ \\text{student} $ (yes/no), and label $ \\text{buys\\_product} $ (yes/no).\n",
    "\n",
    "| age   | income | student | buys_product |\n",
    "|-------|--------|---------|--------------|\n",
    "| young | high   | no      | no           |\n",
    "| young | high   | yes     | yes          |\n",
    "| old   | high   | no      | yes          |\n",
    "| old   | low    | yes     | yes          |\n",
    "| young | low    | no      | no           |\n",
    "| old   | low    | no      | no           |\n",
    "| young | low    | yes     | yes          |\n",
    "| old   | high   | yes     | yes          |\n",
    "\n",
    "**Tasks:**\n",
    "1. Implement entropy calculation.\n",
    "2. Implement information gain calculation.\n",
    "3. Build a tree with a max depth of 3, splitting on the highest information gain.\n",
    "4. Apply pruning: stop splitting if gain < 0.1.\n",
    "5. Predict for $ \\text{age}=\\text{young} $, $ \\text{income}=\\text{low} $, $ \\text{student}=\\text{no} $.\n",
    "\n",
    "**Questions:**\n",
    "- Why was a feature chosen for the root?\n",
    "  - **Answer:** YOUR ANSWER HERE\n",
    "- How does pruning prevent overfitting?\n",
    "  - **Answer:** YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Toy dataset\n",
    "data = np.array([\n",
    "    ['young', 'high', 'no', 'no'],\n",
    "    ['young', 'high', 'yes', 'yes'],\n",
    "    ['old', 'high', 'no', 'yes'],\n",
    "    ['old', 'low', 'yes', 'yes'],\n",
    "    ['young', 'low', 'no', 'no'],\n",
    "    ['old', 'low', 'no', 'no'],\n",
    "    ['young', 'low', 'yes', 'yes'],\n",
    "    ['old', 'high', 'yes', 'yes']\n",
    "])\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "# Entropy calculation\n",
    "def entropy(labels):\n",
    "    ...\n",
    "\n",
    "H_y = entropy(y)\n",
    "print(f'Entropy of buys_product: {H_y}')\n",
    "\n",
    "# Information gain calculation\n",
    "def info_gain(feature_idx, X, y):\n",
    "    ...\n",
    "\n",
    "# Decision Tree Node\n",
    "class TreeNode:\n",
    "    def __init__(self, feature=None, value=None, left=None, right=None, label=None):\n",
    "        self.feature = feature\n",
    "        self.value = value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.label = label\n",
    "\n",
    "# Build Decision Tree\n",
    "def build_tree(X, y, depth=0, max_depth=3, min_gain=0.1):\n",
    "    ...\n",
    "\n",
    "# Predict\n",
    "def predict(tree, x):\n",
    "    ...\n",
    "\n",
    "# Build and predict\n",
    "tree = build_tree(X, y)\n",
    "new_instance = ['young', 'low', 'no']\n",
    "prediction = predict(tree, new_instance)\n",
    "print(f'Prediction for young, low, no: {prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Regression (Polynomial and Logistic)\n",
    "\n",
    "### Theoretical Overview\n",
    "**Polynomial Regression** fits continuous outcomes:\n",
    "\n",
    "$$ y(x, \\mathbf{w}) = \\sum_{j=0}^{M} w_j x^j $$\n",
    "- **Error:** $ E(\\mathbf{w}) = \\frac{1}{2} \\sum_{n=1}^{N} (y(x_n, \\mathbf{w}) - t_n)^2 $\n",
    "- **Regularization:** Adds $ \\frac{\\lambda}{2} ||\\mathbf{w}||^2 $ to prevent overfitting.\n",
    "\n",
    "**Logistic Regression** predicts class probabilities:\n",
    "\n",
    "$$ P(Y=1 | X) = \\frac{1}{1 + \\exp(-(\\mathbf{w}^T X + b))} $$\n",
    "- **Loss:** Cross-entropy.\n",
    "\n",
    "### Practical Exercise: Polynomial and Logistic Regression\n",
    "**Tasks:**\n",
    "1. Fit polynomial regression (degrees 1, 3, 9) to a synthetic dataset and plot.\n",
    "2. Apply Ridge regularization to degree 9 ($ \\lambda = 0.01 $) and plot.\n",
    "3. Implement logistic regression with gradient descent on the Decision Tree dataset.\n",
    "4. Compare polynomial and logistic regression on the classification task.\n",
    "\n",
    "**Questions:**\n",
    "- Why does degree 9 overfit?\n",
    "  - **Answer:** YOUR ANSWER HERE\n",
    "- How does regularization help?\n",
    "  - **Answer:** YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Polynomial Regression\n",
    "np.random.seed(0)\n",
    "X_poly = np.linspace(0, 1, 10).reshape(-1, 1)\n",
    "y_poly = np.sin(2 * np.pi * X_poly).ravel() + np.random.normal(0, 0.1, 10)\n",
    "\n",
    "def polynomial_features(X, degree):\n",
    "    X_poly = np.ones((X.shape[0], 1))\n",
    "    for d in range(1, degree + 1):\n",
    "        X_poly = np.hstack((X_poly, X ** d))\n",
    "    return X_poly\n",
    "\n",
    "def fit_polynomial(X_poly, y, lambda_reg=0):\n",
    "    ...\n",
    "\n",
    "def predict(X_poly, w):\n",
    "    ...\n",
    "\n",
    "def sse(y_true, y_pred):\n",
    "    ...\n",
    "\n",
    "# Polynomial fits\n",
    "degrees = [1, 3, 9]\n",
    "plt.figure(figsize=(12, 4))\n",
    "X_test = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "for i, deg in enumerate(degrees, 1):\n",
    "    X_poly_train = polynomial_features(X_poly, deg)\n",
    "    w = fit_polynomial(X_poly_train, y_poly)\n",
    "    y_pred = predict(X_poly_train, w)\n",
    "    X_test_poly = polynomial_features(X_test, deg)\n",
    "    y_test_pred = predict(X_test_poly, w)\n",
    "    error = sse(y_poly, y_pred)\n",
    "    plt.subplot(1, 4, i)\n",
    "    plt.scatter(X_poly, y_poly, color='blue', label='Data')\n",
    "    plt.plot(X_test, y_test_pred, 'r-', label=f'Degree {deg}')\n",
    "    plt.title(f'Degree {deg}, SSE={error:.2f}')\n",
    "    plt.legend()\n",
    "\n",
    "# Regularized degree 9\n",
    "X_poly_9 = polynomial_features(X_poly, 9)\n",
    "w_reg = fit_polynomial(X_poly_9, y_poly, lambda_reg=0.01)\n",
    "y_pred_reg = predict(X_poly_9, w_reg)\n",
    "X_test_poly_9 = polynomial_features(X_test, 9)\n",
    "y_test_pred_reg = predict(X_test_poly_9, w_reg)\n",
    "error_reg = sse(y_poly, y_pred_reg)\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.scatter(X_poly, y_poly, color='blue', label='Data')\n",
    "plt.plot(X_test, y_test_pred_reg, 'g-', label='Deg 9 Reg')\n",
    "plt.title(f'Degree 9 Reg, SSE={error_reg:.2f}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('regression_plots.png')\n",
    "plt.close()\n",
    "\n",
    "# Logistic Regression\n",
    "X_log = np.array([\n",
    "    [0, 0, 0],  # young, high, no\n",
    "    [0, 0, 1],  # young, high, yes\n",
    "    [1, 0, 0],  # old, high, no\n",
    "    [1, 1, 1],  # old, low, yes\n",
    "    [0, 1, 0],  # young, low, no\n",
    "    [1, 1, 0],  # old, low, no\n",
    "    [0, 1, 1],  # young, low, yes\n",
    "    [1, 0, 1]   # old, high, yes\n",
    "])\n",
    "y_log = np.array([0, 1, 1, 1, 0, 0, 1, 1])\n",
    "\n",
    "def sigmoid(z):\n",
    "    ...\n",
    "\n",
    "def logistic_regression(X, y, lr=0.1, epochs=1000):\n",
    "    ...\n",
    "\n",
    "w_log = logistic_regression(X_log, y_log)\n",
    "new_instance_log = np.array([0, 1, 0])\n",
    "prob = sigmoid(np.dot(new_instance_log, w_log))\n",
    "prediction_log = 1 if prob > 0.5 else 0\n",
    "print(f'Logistic Regression prediction for young, low, no: {prediction_log}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Neural Networks (Perceptron and Multi-Layer Perceptron)\n",
    "\n",
    "### Theoretical Overview\n",
    "**Perceptron:** A linear classifier.\n",
    "**Multi-Layer Perceptron (MLP):** Adds hidden layers for non-linear problems.\n",
    "\n",
    "### Practical Exercise: Implementing Perceptron and MLP\n",
    "**Dataset:**\n",
    "| $x_1$ | $x_2$ | $y$ |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 0 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 1  | 1  | 0 |\n",
    "\n",
    "**Tasks:**\n",
    "1. Implement and train a perceptron.\n",
    "2. Implement and train an MLP with 2 hidden units.\n",
    "3. Compare their performance.\n",
    "\n",
    "**Questions:**\n",
    "- Why can't the perceptron solve XOR?\n",
    "  - **Answer:** YOUR ANSWER HERE\n",
    "- How does the MLP help?\n",
    "  - **Answer:** YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Perceptron\n",
    "class Perceptron:\n",
    "    def __init__(self):\n",
    "        self.w = np.zeros(3)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        ...\n",
    "    \n",
    "    def train(self, X, y, epochs=10):\n",
    "        ...\n",
    "\n",
    "perceptron = Perceptron()\n",
    "perceptron.train(X, y)\n",
    "print('Perceptron predictions:', [perceptron.predict(x) for x in X])\n",
    "\n",
    "# MLP\n",
    "class MLP:\n",
    "    def __init__(self, hidden_size=2, lr=0.5):\n",
    "        self.lr = lr\n",
    "        self.W1 = np.random.randn(2, hidden_size) * 0.1\n",
    "        self.b1 = np.zeros(hidden_size)\n",
    "        self.W2 = np.random.randn(hidden_size, 1) * 0.1\n",
    "        self.b2 = np.zeros(1)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        ...\n",
    "    \n",
    "    def sigmoid_deriv(self, x):\n",
    "        ...\n",
    "    \n",
    "    def forward(self, X):\n",
    "        ...\n",
    "    \n",
    "    def train(self, X, y, epochs=10000):\n",
    "        ...\n",
    "    \n",
    "    def predict(self, X):\n",
    "        ...\n",
    "\n",
    "mlp = MLP()\n",
    "mlp.train(X, y)\n",
    "print('MLP predictions:', mlp.predict(X).flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: CNN From Scratch!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CNN__ s are powerfull tools to work with in the scope of _Computer Vision_. You have seen MLPs before and know how to do forward and backward pass on them. In this part, we want to create a convolutional layer (a simple one, of course!) just to understand well that how a Convolutional Layer works, behind the scenes. You can read more about Convolutional Neural Networks in this [Link](https://medium.com/thedeephub/convolutional-neural-networks-a-comprehensive-guide-5cc0b5eae175)\n",
    "\n",
    "\n",
    "\n",
    "__FORWARD PASS__:\n",
    "\n",
    "In forward pass, you should apply the convolution operation on the input image. The convolution operation is as follows:\n",
    "\n",
    "$$\n",
    "\\text{output}[i, j] = \\sum_{k=0}^{K-1} \\sum_{l=0}^{L-1} \\text{input}[i+k, j+l] \\times \\text{kernel}[k, l]\n",
    "$$\n",
    "\n",
    "You can see a sample convolution operation (with a $3 \\times 3$ kernel) in the following image:\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"./Images/Conv.gif\" /> \n",
    "</div>\n",
    "\n",
    "Please note that, these are samples for you to understand the operation better. For _forward pass_ that you should implement, please notice that you should convolve the kernel along all $C$ channels of the input image. So, the output shape should be $(N, H_{out}, W_{out})$. These values are calculated as follows:\n",
    "\n",
    "$$\n",
    "H_{out} = \\frac{H_{in} + 2 \\times \\text{padding}}{\\text{stride} - HH} + 1\n",
    "$$\n",
    "$$\n",
    "W_{out} = \\frac{W_{in} + 2 \\times \\text{padding}}{\\text{stride} - WW} + 1\n",
    "$$\n",
    "\n",
    "\n",
    "where $HH$ and $WW$ are the height and width of the kernel, respectively. __stride__ is the step size of the kernel, and __padding__ is the number of zeros that should be padded to the input image. Please also note that $b$ stands for bias, which is a scalar value that should be added to the output of the convolution operation. (for each kernel)\n",
    "\n",
    "You can see a more detailed example, below:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"./Images/conv3.gif\" /> \n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Backward Pass__:\n",
    "\n",
    "In backward pass, you should calculate gradient of output with respect to the input image and the kernel. These gradients will be used in _optimization_ to update kernel weights. (You saw something similar, in using MLPs with gradient descent algorithm!). So, we need $d_x$, $d_w$ and $d_b$ in the backward pass. These are calculated as follows:\n",
    "\n",
    "$$\n",
    "dx_{padded} [n, :, i * stride: i * stride + HH, j * stride: j * stride + WW] += w[f] \\times dout[n, f, i, j]\n",
    "$$\n",
    "\n",
    "$$\n",
    "dw_{f} = \\sum_{n=0}^{N - 1} \\sum_{i=0}^{H_{out} - 1} \\sum_{j=0}^{W_{out} - 1} x_{padded}[n, :, i * stride: i * stride + HH, j * stride: j * stride + WW] \\times dout[n, f, i, j]  \n",
    "$$\n",
    "\n",
    "$$\n",
    "db_{f} = \\sum_{n=0}^{N - 1} \\sum_{i=0}^{H_{out} - 1} \\sum_{j=0}^{W_{out} - 1} dout[n, f, i, j]\n",
    "$$\n",
    "\n",
    "For better understanding of the backward pass, you can see the following image:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"./Images/backprop_cs231n.png\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class MyConv:\n",
    "    def __init__(self, stride, padding):\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, x, w, b):\n",
    "        out = None\n",
    "\n",
    "        ####### TO DO : Implement Forward pass of Conv2D #######\n",
    "        ########################################################\n",
    "\n",
    "        N, C, H, W = ...\n",
    "        F, C, HH, WW = ...\n",
    "        pad = self.padding\n",
    "        stride = self.stride\n",
    "        H_out = ...\n",
    "        W_out = ...\n",
    "\n",
    "        x_pad = ...\n",
    "        out = ...\n",
    "\n",
    "        for n in range(N):\n",
    "            for f in range(F):\n",
    "                for k in range(H_out):\n",
    "                    for l in range(W_out):\n",
    "                        out[n, f, k, l] = ...\n",
    "\n",
    "        ####### TO DO : End #######\n",
    "        #############################\n",
    "\n",
    "        cache = (x, w, b, x_pad)\n",
    "        self.cache = cache\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx, dw, db = None, None, None\n",
    "\n",
    "        x, w, b, x_padded = self.cache\n",
    "\n",
    "        ####### TO DO : Implement backward pass #######\n",
    "        ###############################################\n",
    "\n",
    "        N, C, H, W = ...\n",
    "        F, C, HH, WW = ...\n",
    "        N, F, H_OUT, W_OUT = ...\n",
    "\n",
    "        dw = ...\n",
    "        dx = ...\n",
    "        db = ...\n",
    "\n",
    "        dx_padded = ...\n",
    "\n",
    "        for n in range(N):\n",
    "            for f in range(F):\n",
    "                for i in range(H_OUT):\n",
    "                    for j in range(W_OUT):\n",
    "                        dw[f] += ...\n",
    "                        dx_padded[\n",
    "                            n,\n",
    "                            :,\n",
    "                            i * self.stride : i * self.stride + HH,\n",
    "                            j * self.stride : j * self.stride + WW,\n",
    "                        ] += ...\n",
    "                        db[f] += ...\n",
    "\n",
    "        dx = ...\n",
    "        db = ...\n",
    "\n",
    "        ####### TO DO : End #######\n",
    "        #############################\n",
    "\n",
    "        self.dx = dx\n",
    "        self.dw = dw\n",
    "        self.db = db\n",
    "        return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE ######\n",
    "def rel_error(x, y):\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple test for forward pass (DO NOT CHANGE)\n",
    "\n",
    "x_shape = (2, 3, 4, 4)\n",
    "w_shape = (3, 3, 4, 4)\n",
    "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
    "b = np.linspace(-0.1, 0.2, num=3)\n",
    "\n",
    "out = MyConv(stride=2, padding=1).forward(x, w, b)\n",
    "correct_out = np.array([[[[[-0.08759809, -0.10987781],\n",
    "                           [-0.18387192, -0.2109216 ]],\n",
    "                          [[ 0.21027089,  0.21661097],\n",
    "                           [ 0.22847626,  0.23004637]],\n",
    "                          [[ 0.50813986,  0.54309974],\n",
    "                           [ 0.64082444,  0.67101435]]],\n",
    "                         [[[-0.98053589, -1.03143541],\n",
    "                           [-1.19128892, -1.24695841]],\n",
    "                          [[ 0.69108355,  0.66880383],\n",
    "                           [ 0.59480972,  0.56776003]],\n",
    "                          [[ 2.36270298,  2.36904306],\n",
    "                           [ 2.38090835,  2.38247847]]]]])\n",
    "\n",
    "# The outputted difference which is printed, should be around 1e-8\n",
    "print ('Testing conv_forward_naive')\n",
    "print ('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
    "  grad = np.zeros_like(x)\n",
    "  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "  while not it.finished:\n",
    "    ix = it.multi_index\n",
    "    \n",
    "    oldval = x[ix]\n",
    "    x[ix] = oldval + h\n",
    "    pos = f(x).copy()\n",
    "    x[ix] = oldval - h\n",
    "    neg = f(x).copy()\n",
    "    x[ix] = oldval\n",
    "    \n",
    "    grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "    it.iternext()\n",
    "  return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple test for backward pass (DO NOT CHANGE)\n",
    "\n",
    "x = np.random.randn(4, 3, 5, 5)\n",
    "w = np.random.randn(2, 3, 3, 3)\n",
    "b = np.random.randn(\n",
    "    2,\n",
    ")\n",
    "dout = np.random.randn(4, 2, 5, 5)\n",
    "conv = MyConv(stride=1, padding=1)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(\n",
    "    lambda x: conv.forward(x, w, b), x, dout\n",
    ")\n",
    "dw_num = eval_numerical_gradient_array(\n",
    "    lambda w: conv.forward(x, w, b), w, dout\n",
    ")\n",
    "db_num = eval_numerical_gradient_array(\n",
    "    lambda b: conv.forward(x, w, b), b, dout\n",
    ")\n",
    "\n",
    "out = conv.forward(x, w, b)\n",
    "dx, dw, db = conv.backward(dout)\n",
    "\n",
    "# Your printed errors should be around 1e-9\n",
    "print(\"Testing conv_backward_naive function\")\n",
    "print(\"dx error: \", rel_error(dx, dx_num))\n",
    "print(\"dw error: \", rel_error(dw, dw_num))\n",
    "print(\"db error: \", rel_error(db, db_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we try to visualize the results of the convolution operation. We will 2 sample photos (in `Images/Sample`) and then we will use our `MyConv` class to apply some cool convolutions (and see their result on an image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "first_img, second_img = Image.open(\"./Images/Sample/image_1.JPG\"), Image.open(\n",
    "    \"./Images/Sample/image_2.jpg\"\n",
    ")\n",
    "first_img = first_img.resize((256, 256))\n",
    "second_img = second_img.resize((256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutions have some interesting usages in image processing. For example, you can extract edges from an image by applying a convolution with a kernel that detects edges. Or you can convolve a specific kernel with your image to make it grayscale or blurred! Look at the definitions below:\n",
    "\n",
    "__Edge Detection Kernel__:\n",
    "\n",
    "_Sobel_ is one of the most famous edge detection kernels. It has two kernels, one for detecting vertical edges and the other for horizontal edges. You can see the kernels below:\n",
    "$$\n",
    "\\text{Sobel}_x = \\begin{bmatrix}\n",
    "-1 & 0 & 1 \\\\\n",
    "-2 & 0 & 2 \\\\\n",
    "-1 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Sobel}_y = \\begin{bmatrix}\n",
    "-1 & -2 & -1 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "1 & 2 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "__Grayscale Kernel__:\n",
    "\n",
    "You can simply convolve the image with the following kernel to make it grayscale:\n",
    "\n",
    "$$\n",
    "\\text{Grayscale\\_across\\_R} = \\begin{bmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 0.3 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Grayscale\\_across\\_G} = \\begin{bmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 0.6 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Grayscale\\_across\\_B} = \\begin{bmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 0.1 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "(Convolve each kernel with its corresponding channel)\n",
    "\n",
    "__Blurring Kernel__:\n",
    "\n",
    "One of the famous kernels to blur an image is _Gaussian Blur_. You can see the kernel below:\n",
    "\n",
    "$$\n",
    "\\text{Gaussian\\_Blur} = \\frac{1}{16} \\times \\begin{bmatrix}\n",
    "1 & 2 & 1 \\\\\n",
    "2 & 4 & 2 \\\\\n",
    "1 & 2 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "\n",
    "x = np.zeros((2, 3, img_size, img_size))\n",
    "x[0] = np.array(first_img).transpose(2, 0, 1)\n",
    "x[1] = np.array(second_img).transpose(2, 0, 1)\n",
    "\n",
    "# a convolution weight, holding 4 filters 3x3\n",
    "w = ...\n",
    "\n",
    "b = np.array([0, 128, 128, 0])\n",
    "\n",
    "conv = MyConv(stride=1, padding=1)\n",
    "out = conv.forward(x, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#TODO: Plot the original image, grayscale, sobel x, sobel y, and gaussian blur of each image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
